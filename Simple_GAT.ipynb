{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packages to import\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch_geometric.utils import from_networkx\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from itertools import combinations, product\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate graph data object\n",
    "\n",
    "def flatten_node_attributes(node_attr):\n",
    "        \"\"\"Flatten the node attributes to ensure all values are numeric.\"\"\"\n",
    "        flat_attrs = []\n",
    "        for attr in node_attr.values():\n",
    "            if isinstance(attr, (tuple, list)):\n",
    "                flat_attrs.extend(attr)\n",
    "            else:\n",
    "                flat_attrs.append(attr)\n",
    "        return flat_attrs\n",
    "    \n",
    "\n",
    "def load_graph_data(node_features_file, edge_features_file):\n",
    "    node_features_df = pd.read_csv(node_features_file)\n",
    "    edge_features_df = pd.read_csv(edge_features_file)\n",
    "\n",
    "    node_features_df['pos'] = node_features_df['pos'].apply(eval).apply(lambda x: tuple(map(float, x)))\n",
    "    node_features_df['dir'] = node_features_df['dir'].apply(eval).apply(lambda x: tuple(map(float, x)))\n",
    "    node_features_df[['ends0', 'ends1']] = node_features_df['ends'].apply(eval).apply(pd.Series)\n",
    "    node_features_df['ends0'] = node_features_df['ends0'].apply(lambda x: tuple(map(float, x)))\n",
    "    node_features_df['ends1'] = node_features_df['ends1'].apply(lambda x: tuple(map(float, x)))\n",
    "\n",
    "     # Select required columns and ensure they are numeric\n",
    "    include_columns = ['stepNum', 'id', 'ObjectNumber', 'parent_id', 'label', 'cellType', 'divideFlag',\n",
    "                       'LifeHistory', 'startVol', 'targetVol', 'radius', 'length', 'strainRate', 'strainRate_rolling',\n",
    "                       'pos', 'dir', 'ends0', 'ends1']\n",
    "    node_features_df = node_features_df[include_columns]\n",
    "\n",
    "   # display(node_features_df)\n",
    "    # Create the graph\n",
    "    G = nx.MultiDiGraph()\n",
    "\n",
    "    # Add nodes with their features as attributes and label them by their index\n",
    "    for idx, row in node_features_df.iterrows():\n",
    "        node_id = idx  # Use the index as a unique label for each node\n",
    "        G.add_node(node_id, **row.to_dict())\n",
    "\n",
    "    # Define positions for the entire graph\n",
    "    pos = {node: data['pos'] for node, data in G.nodes(data=True)}\n",
    "\n",
    "    # Create a unique mapping from (stepNum, ObjectNum) to node index\n",
    "    node_mapping = {(row['stepNum'], row['ObjectNumber']): idx for idx, row in node_features_df.iterrows()}\n",
    "    contact_edges = []\n",
    "    # Add edges, considering contact edges\n",
    "    for _, row in edge_features_df.iterrows():\n",
    "      if row['Relationship'] == 'Neighbors':\n",
    "        step_num_1 = row['First Image Number']\n",
    "        step_num_2 = row['Second Image Number']\n",
    "        node1 = row['First Object Number']\n",
    "        node2 = row['Second Object Number']\n",
    "\n",
    "        if (step_num_1, node1) in node_mapping and (step_num_2, node2) in node_mapping:\n",
    "          if not G.has_edge(node_mapping[(step_num_1, node1)], node_mapping[(step_num_2, node2)]):\n",
    "            node1_idx = node_mapping[(step_num_1, node1)]\n",
    "            node2_idx = node_mapping[(step_num_2, node2)]\n",
    "            G.add_edge(node1_idx, node2_idx, edge_type = 'contact')\n",
    "            G.add_edge(node2_idx, node1_idx, edge_type = 'contact')\n",
    "            contact_edges.append((node1_idx, node2_idx))\n",
    "            contact_edges.append((node2_idx, node1_idx))\n",
    "\n",
    "    # Create lineage mapping\n",
    "    lineage_mapping = {(row['id'], row['parent_id'], row['stepNum']): idx for idx, row in node_features_df.iterrows()}\n",
    "    lineage_edges = []\n",
    "    # Add directed lineage edges based on the lineage mapping\n",
    "    for key, node1_idx in lineage_mapping.items():\n",
    "        id, parent_id, step_num = key\n",
    "        if parent_id == 0:\n",
    "            continue  # Skip if parent_id is zero\n",
    "        if (id, id, step_num + 1) in lineage_mapping:\n",
    "            node2_idx = lineage_mapping[(id, id, step_num + 1)]\n",
    "            G.add_edge(node1_idx, node2_idx, edge_type='lineage')\n",
    "            lineage_edges.append((node1_idx, node2_idx))\n",
    "        elif (id, parent_id, step_num + 1) in lineage_mapping:\n",
    "            node2_idx = lineage_mapping[(id, parent_id, step_num + 1)]\n",
    "            G.add_edge(node1_idx, node2_idx, edge_type='lineage')\n",
    "            lineage_edges.append((node1_idx, node2_idx))\n",
    "        else:\n",
    "            for parent_key, node2_idx in lineage_mapping.items():\n",
    "                parent_id_key, _, step_num_key = parent_key\n",
    "                if parent_id_key == parent_id and step_num_key == step_num - 1:\n",
    "                    G.add_edge(node2_idx, node1_idx, edge_type='lineage')\n",
    "                    lineage_edges.append((node2_idx, node1_idx))\n",
    "                    break\n",
    "\n",
    "\n",
    "    x = torch.tensor([flatten_node_attributes(G.nodes[node]) for node in G.nodes()], dtype=torch.float)\n",
    "    contact_edge_index = torch.tensor(contact_edges, dtype=torch.long).t().contiguous()\n",
    "    lineage_edge_index = torch.tensor(lineage_edges, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    data =  Data(x, contact_edge_index=contact_edge_index, lineage_edge_index=lineage_edge_index)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet Loss functions\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = F.pairwise_distance(anchor, positive, p=2)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative, p=2)\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "class MixedTripletLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, margin=1.0, margin_cosine=0.2):\n",
    "        super(MixedTripletLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.margin = margin\n",
    "        self.margin_cosine = margin_cosine\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        triplet_loss = TripletLoss(self.margin)\n",
    "        triplet_loss_cosine = TripletLossCosine(self.margin_cosine)\n",
    "\n",
    "        loss_triplet = triplet_loss(anchor, positive, negative)\n",
    "        loss_triplet_cosine = triplet_loss_cosine(anchor, positive, negative)\n",
    "\n",
    "        mixed_loss = loss_triplet + self.alpha * loss_triplet_cosine\n",
    "\n",
    "        return mixed_loss\n",
    "\n",
    "class TripletLossCosine(nn.Module):\n",
    "    def __init__(self, margin=0.2):\n",
    "        super(TripletLossCosine, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        distance_positive = 1 - F.cosine_similarity(anchor, positive, dim=-1)\n",
    "        distance_negative = 1 - F.cosine_similarity(anchor, negative, dim=-1)\n",
    "        losses = torch.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder of the data generated by sims_relationships.ipynb\n",
    "def get_file_names(directory):\n",
    "\n",
    "    # Collect files starting with \"Trackrefiner\"\n",
    "    matching_files = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.startswith(\"Trackrefiner\"):\n",
    "                matching_files.append(os.path.join(root, file))\n",
    "\n",
    "    return matching_files\n",
    "\n",
    "#Identifying parameter combinations\n",
    "\n",
    "def identify_parameter_combinations(matching_files):\n",
    "    variations_with_instances = np.unique(list(map(lambda x: x.split('Trackrefiner.')[1].split('result')[0], matching_files)))\n",
    "\n",
    "    # Initialize sets to collect unique values\n",
    "    unique_gamma = set()\n",
    "    unique_reg_param = set()\n",
    "    unique_adh = set()\n",
    "    unique_average = set()\n",
    "\n",
    "\n",
    "    # Extract parameter values\n",
    "    for var in variations_with_instances:\n",
    "        # Remove the instance part\n",
    "        base_var, average = var.split('-Average-')\n",
    "        uni_average = average[0]\n",
    "        # Parse the gamma, reg_param, and adh values\n",
    "        parts = base_var.split('_')\n",
    "        gamma = parts[1]\n",
    "        reg_param = parts[4]\n",
    "        adh = parts[6]\n",
    "\n",
    "        # Add to respective sets\n",
    "        unique_gamma.add(gamma)\n",
    "        unique_reg_param.add(reg_param)\n",
    "        unique_adh.add(adh)\n",
    "        unique_average.add(uni_average)\n",
    "\n",
    "    # Convert to sorted lists (optional)\n",
    "    unique_gamma = list(sorted(unique_gamma))\n",
    "    unique_reg_param = list(sorted(unique_reg_param))\n",
    "    unique_adh = list(sorted(unique_adh))\n",
    "    return unique_gamma, unique_reg_param, unique_adh\n",
    "\n",
    "# Generate the data structure\n",
    "def get_paramter_set_simulations(gamma_values, reg_param_values, adh_values):\n",
    "    data = {\n",
    "        f\"gamma_{gamma}_reg_param_{reg_param}_adh_{adh}\": [\n",
    "            f\"gamma_{gamma}_reg_param_{reg_param}_adh_{adh}-Average-{avg}\" \n",
    "            for avg in average_values\n",
    "        ]\n",
    "        for gamma, reg_param, adh in product(gamma_values, reg_param_values, adh_values)\n",
    "    }\n",
    "    return data\n",
    "\n",
    "#this function will have a simulation instance as key and the values as [node_features, node_relationships]\n",
    "def generate_simulation_ditionary(file_list):\n",
    "    # Regex pattern to extract parameter combinations and Average instance\n",
    "    pattern = re.compile(r\"gamma_(\\d+)_reg_param_(\\d+\\.?\\d*)_adh_(\\d+\\.?\\d*)-Average-(\\d+)\")\n",
    "\n",
    "    # Dictionary to store the key-value pairs\n",
    "    parameter_dict = defaultdict(list)\n",
    "\n",
    "    # Process each filename\n",
    "    for file in file_list:\n",
    "        match = pattern.search(file)\n",
    "        if match:\n",
    "            # Extract values\n",
    "            gamma, reg_param, adh, average = match.groups()\n",
    "            # Create key as parameter combination\n",
    "            key = f\"gamma_{gamma}_reg_param_{reg_param}_adh_{adh}-Average-{average}\"\n",
    "            # Append the Average instance to the key's list\n",
    "            parameter_dict[key] = [file, file.split('Trackrefiner')[0] + 'ObjectRelationship.Trackrefiner' + file.split('Trackrefiner')[1]]\n",
    "\n",
    "    # Convert defaultdict to a regular dictionary\n",
    "    parameter_dict = dict(parameter_dict)\n",
    "    return parameter_dict\n",
    "\n",
    "# Generate triplets\n",
    "def generate_triplets(data):\n",
    "    triplets = []\n",
    "    params = list(data.keys())  # List of parameter sets\n",
    "\n",
    "    for param, instances in data.items():\n",
    "        # Anchor and Positive from the same parameter set\n",
    "        for anchor, positive in combinations(instances, 2):\n",
    "            # Negative from a different parameter set\n",
    "            negative_param = random.choice([p for p in params if p != param])\n",
    "            negative = random.choice(data[negative_param])\n",
    "            triplets.append((anchor, positive, negative))\n",
    "\n",
    "    return triplets\n",
    "\n",
    "\n",
    "#triplet batches\n",
    "class TripletDataset(Dataset):\n",
    "    def __init__(self, triplets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            triplets: A list of triplets (anchor_data, positive_data, negative_data)\n",
    "        \"\"\"\n",
    "        self.triplets = triplets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_data, positive_data, negative_data = self.triplets[idx]\n",
    "        return anchor_data, positive_data, negative_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAT Block with Attention Mechanism\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels=22, out_channels = hidden_channels, heads=2)\n",
    "        self.conv2 = GATConv(hidden_channels * 2, out_channels=out_channels, heads=1, concat=False)\n",
    "\n",
    "        self.conv3 = GATConv(out_channels, out_channels=hidden_channels, heads=2)\n",
    "        self.conv4 = GATConv(hidden_channels * 2, out_channels=out_channels, heads=1, concat=False)\n",
    "\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, c_edge_index, l_edge_index = data.x, data.contact_edge_index, data.lineage_edge_index\n",
    "        x = self.conv1(x, c_edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv2(x, c_edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = self.conv3(x, l_edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = self.conv4(x, l_edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        x = global_mean_pool(x, data.batch)\n",
    "        return x\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training on data\n",
    "def train(data_loader, device, model, criterion, optimizer, num_epochs=20):\n",
    "    \"\"\"\n",
    "    Train the model using triplet loss.\n",
    "\n",
    "    Args:\n",
    "        data_loader: DataLoader that loads the triplet data.\n",
    "        model: The model to be trained (e.g., GAT).\n",
    "        criterion: The loss function (e.g., TripletMarginLoss).\n",
    "        optimizer: The optimizer (e.g., Adam).\n",
    "        num_epochs: The number of training epochs.\n",
    "    \"\"\"\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Initialize tqdm for progress bar and display the epoch\n",
    "        epoch_progress = tqdm(data_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\", ncols=100)\n",
    "\n",
    "        for anchor_data, positive_data, negative_data in epoch_progress:\n",
    "            # Transfer the data to the appropriate device (GPU or CPU)\n",
    "            anchor_data, positive_data, negative_data = anchor_data.to(device), positive_data.to(device), negative_data.to(device)\n",
    "\n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass for anchor, positive, and negative data\n",
    "            anchor_out = model(anchor_data)  # Process anchor data\n",
    "            positive_out = model(positive_data)  # Process positive data\n",
    "            negative_out = model(negative_data)  # Process negative data\n",
    "\n",
    "            # Calculate the triplet loss\n",
    "            loss = criterion(anchor_out, positive_out, negative_out)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the loss summary and progress bar description\n",
    "            running_loss += loss.item()\n",
    "            avg_loss = running_loss / (epoch_progress.n + 1)\n",
    "            epoch_progress.set_postfix(loss=avg_loss)\n",
    "\n",
    "        avg_loss = running_loss / len(data_loader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check model performance\n",
    "def validate(data_loader, model, criterion, margin):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0.0\n",
    "    correct_triplets = 0\n",
    "    total_triplets = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for anchor_data, positive_data, negative_data in data_loader:\n",
    "            # Transfer data to the device\n",
    "            anchor_data = anchor_data.to(device)\n",
    "            positive_data = positive_data.to(device)\n",
    "            negative_data = negative_data.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            anchor_out = model(anchor_data)\n",
    "            positive_out = model(positive_data)\n",
    "            negative_out = model(negative_data)\n",
    "\n",
    "            # Calculate triplet loss\n",
    "            loss = criterion(anchor_out, positive_out, negative_out)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Evaluate triplet condition\n",
    "            distance_positive = F.pairwise_distance(anchor_out, positive_out, p=2)\n",
    "            distance_negative = F.pairwise_distance(anchor_out, negative_out, p=2)\n",
    "\n",
    "            correct_triplets += (distance_positive + margin < distance_negative).sum().item()\n",
    "            total_triplets += len(distance_positive)\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = correct_triplets / total_triplets * 100\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}, Triplet Accuracy: {accuracy:.2f}%\")\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split training data\n",
    "#the split should sum upto 1\n",
    "def generate_data_splits(train_split, test_split, val_split, triplets):\n",
    "    if train_split + test_split + val_split == 1:\n",
    "        \n",
    "        #processing of triplet file names\n",
    "        triplets_n = []\n",
    "        for i in triplets:\n",
    "            temp = []\n",
    "            for j in i:\n",
    "                temp.append(j.split('-')[0])\n",
    "            triplets_n.append(tuple(temp))\n",
    "            \n",
    "        # Step 1: Create a mapping of anchor-negative pairs\n",
    "        anchor_negative_map = defaultdict(list)\n",
    "        k = 0\n",
    "        for anchor, positive, negative in triplets_n:\n",
    "            anchor_negative_map[(anchor, negative)].append((k, anchor, positive, negative))\n",
    "            k+=1\n",
    "\n",
    "        # Step 2: Get anchor-negative pairs\n",
    "        anchor_negative_pairs = list(anchor_negative_map.keys())\n",
    "\n",
    "        # Step 3: Stratified split for training, validation, and test\n",
    "        train_pairs, temp_pairs = train_test_split(\n",
    "            anchor_negative_pairs, test_size=1-train_split, random_state=42\n",
    "        )\n",
    "        val_pairs, test_pairs = train_test_split(\n",
    "            temp_pairs, test_size=val_split/(1-train_split), random_state=42\n",
    "        )\n",
    "\n",
    "        # Step 4: Collect triplets corresponding to each split\n",
    "        train_triplets = [triplet for pair in train_pairs for triplet in anchor_negative_map[pair]]\n",
    "        val_triplets = [triplet for pair in val_pairs for triplet in anchor_negative_map[pair]]\n",
    "        test_triplets = [triplet for pair in test_pairs for triplet in anchor_negative_map[pair]]\n",
    "        \n",
    "        train_ix = list(map(lambda x: x[0], train_triplets))\n",
    "        val_ix = list(map(lambda x: x[0], val_triplets))\n",
    "        test_ix = list(map(lambda x: x[0],test_triplets))\n",
    "        \n",
    "        triplets = np.array(triplets)\n",
    "        train_dat = triplets[train_ix]\n",
    "        test_dat = triplets[test_ix]\n",
    "        val_dat = triplets[val_ix]\n",
    "        \n",
    "        final_train_data = []\n",
    "        final_test_data = []\n",
    "        final_val_data = []\n",
    "\n",
    "        for i in train_dat:\n",
    "            k = []\n",
    "            for j in i:\n",
    "                k.append(data_tuple[j])\n",
    "            final_train_data.append(k)\n",
    "\n",
    "        for i in test_dat:\n",
    "            k = []\n",
    "            for j in i:\n",
    "                k.append(data_tuple[j])\n",
    "            final_test_data.append(k)\n",
    "            \n",
    "        for i in val_dat:\n",
    "            k = []\n",
    "            for j in i:\n",
    "                k.append(data_tuple[j])\n",
    "            final_val_data.append(k)\n",
    "            \n",
    "        return final_train_data, final_test_data, final_val_data    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-configurable parameters\n",
    "n_simulation_instances = 10 #number of simulation instances for every parameter combination generated from ABM\n",
    "#configurable parameters\n",
    "train_split =0.7\n",
    "test_split = 0.15\n",
    "val_split = 0.15\n",
    "batch_train_size = 32\n",
    "hidden_channels_model = 4 #attention heads\n",
    "feature_embedding_size = 5\n",
    "model_learning_rate = 0.001 #optimizer can be changed and can also be learnt using adaptive learning rates\n",
    "mixed_loss_alpha = 0.5\n",
    "mixed_loss_margin = 1\n",
    "mixed_loss_cosine = 0.2\n",
    "model_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Triplets generation\n",
    "directory = \"/Users/sushmadhamodharan/Downloads/finaldata\"\n",
    "matching_files = get_file_names(directory)\n",
    "unique_gamma, unique_reg_param, unique_adh = identify_parameter_combinations(matching_files)\n",
    "n_simulation_instances = 10 #number of simulation instances for every parameter combination generated from ABM\n",
    "average_values = [str(i) for i in range(n_simulation_instances)]\n",
    "data = get_paramter_set_simulations(unique_gamma, unique_reg_param, unique_adh)\n",
    "# Generate triplets\n",
    "triplets = generate_triplets(data)\n",
    "# Example list of filenames\n",
    "file_list = matching_files\n",
    "parameter_dict = generate_simulation_ditionary(file_list)\n",
    "\n",
    "#generating the graph object for every simulation\n",
    "data_tuple = {}\n",
    "for key in parameter_dict.keys():\n",
    "    data_tuple[key] = load_graph_data(parameter_dict[key][0], parameter_dict[key][1])\n",
    "\n",
    "final_train_data, final_test_data, final_val_data = generate_data_splits(train_split, test_split, val_split, triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|█████████████████████████████████████| 98/98 [01:08<00:00,  1.44batch/s, loss=0.46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████| 98/98 [01:08<00:00,  1.44batch/s, loss=0.322]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.3220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████| 98/98 [01:06<00:00,  1.48batch/s, loss=0.298]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.2978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████| 98/98 [01:05<00:00,  1.49batch/s, loss=0.278]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.2781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████| 98/98 [01:07<00:00,  1.45batch/s, loss=0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.2710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████| 98/98 [01:08<00:00,  1.43batch/s, loss=0.258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.2582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████| 98/98 [01:05<00:00,  1.49batch/s, loss=0.249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.2486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████| 98/98 [01:05<00:00,  1.50batch/s, loss=0.242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 0.2424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████| 98/98 [01:20<00:00,  1.21batch/s, loss=0.236]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████| 98/98 [01:09<00:00,  1.41batch/s, loss=0.237]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 0.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "#generate batches of training data\n",
    "triplet_dataset = TripletDataset(final_train_data)\n",
    "triplet_train_dataloader = DataLoader(triplet_dataset, batch_size=batch_train_size, shuffle=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GAT(in_channels = 22,hidden_channels=hidden_channels_model, out_channels=feature_embedding_size).to(device)  # Adjust input/output as needed\n",
    "optimizer = Adam(model.parameters(), lr=model_learning_rate)\n",
    "\n",
    "# Choose the loss function (e.g., MixedTripletLoss, TripletLossCosine, or TripletLoss)\n",
    "criterion = MixedTripletLoss(alpha=mixed_loss_alpha, margin=mixed_loss_margin, margin_cosine=mixed_loss_cosine).to(device)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "train(triplet_train_dataloader, device, model, criterion, optimizer, num_epochs=model_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2193, Triplet Accuracy: 79.39%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.21929494740308975, 79.39042089985486)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(final_test_data, model, criterion, margin = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.2080, Triplet Accuracy: 79.53%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.20796928657918334, 79.52871870397644)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate(final_val_data, model, criterion, margin = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate the feature embeddin using the model and save it in pickle format to use it for visualization in SIMPLE_GAT_visulaization.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
